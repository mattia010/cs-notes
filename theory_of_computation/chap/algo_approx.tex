\section{Problemi di ottimizzazione}
\subsection{Algoritmi di approssimazione}
Un algoritmo di approssimazione è un particolare tipo di algoritmo che trova una soluzione sub-ottimale a un \textbf{problema di ottimizzazione} \footnote{Molto spesso, però, un problema di decisione può essere formulato come un problema di ottimizzazione, e vi si può quindi applicare un algoritmo di approssimazione. Vedi: problema Max SAT.}.
Sono usati sia per ottenere una soluzione approssimata per un problema NP-hard \footnote[][1cm]{I problemi NP-hard sono intrattabili, ovvero non sono conosciuti algoritmi che li risolvono in tempo polinomiale.} in tempo polinomiale, sia per trattare problemi per cui si conosce un algoritmo polinomiale, ma l'algoritmo risulta inapplicabile su istanze di grandi dimensioni.

Gli algoritmi di approssimazione si differenziano dagli algoritmi euristici: i primi, infatti, sono basati su limiti dimostrabili alla qualità della soluzione e ai tempi di esecuzione, mentre i secondi si basano su criteri non dimostrati e assunti veri.

\subsection*{Introduzione}
\begin{defn}
    Un \textbf{algoritmo di approssimazione} è un algoritmo efficiente che trova soluzioni approssimate a un problema di ottimizzazione, fornendo limiti dimostrabili alla qualità della soluzione ritornata.\\
    Con qualità della soluzione approssimata si intende la sua distanza dalla soluzione ottima.
\end{defn}

\marginnote{Non tutti i problemi di ottimizzazione ammettono algoritmi $f(n)$-approssimanti. Un esempio di problema non approssimabile è la ricerca dell'insieme indipendente in un grafo.}

\begin{defn}
    Un algoritmo di approssimazione per un problema $\Pi$ si dice \textbf{$f(n)$-approssimante}, con $f(n)$ funzione dipendente dalla dimensione dell'input $n$, se la soluzione $\appr(x)$ trovata dall'algoritmo dista dalla soluzione ottima $\opt(x)$ al massimo per un fattore moltiplicativo $f(n)$.
    $f(n)$ prende il nome di \textbf{rapporto di approssimazione} o garanzia di prestazione relativa.
    \[
    \begin{cases}
        \opt(x) \le \appr(x) \le f(n) \cdot \opt(x) & \tn{problemi di minimo}\\
        f(n) \cdot \opt(x) \le \appr(x) \le \opt(x) & \tn{problemi di massimo}
    \end{cases}
    \]
\end{defn}

\begin{rem}
    La $f(n)$-approssimazione non pone alcun vincolo sul tempo di esecuzione dell'algoritmo di approssimazione, ma solo sulla qualità della sua soluzione.\\
    Ad esempio, con algoritmo $2$-approssimante si può intendere sia un algoritmo polinomiale che un algoritmo esponenziale. 
\end{rem}

\subsection*{Classi di complessità di approssimazione}
I problemi di ottimizzazione approssimati, così come i problemi di decisione, possono essere organizzati in classi di complessità di approssimazione.

\begin{defn}
    \textbf{APX} è la classe dei problemi di ottimizzazione la cui versione di decisione appartiene a NP e che ammettono un algoritmo di approssimazione in tempo polinomiale, con rapporto di approssimazione costante.
\end{defn}

\begin{defn}
    Un problema di approssimazione $\Pi$ è un \textbf{Polynomial-Time Approxiamtion Scheme} (PTAS) sse per ogni rapporto di approssimazione costante $\epsilon > 1$, esiste un algoritmo $\epsilon$-approssimante che risolve $\Pi$ in tempo polinomiale.
\end{defn}

\begin{rem}
    Se un problema ammette un algoritmo $k$-approssimante in tempo polinomiale, allora ammette anche algoritmi $\epsilon$-approssimanti in tempo polinomiale, $\forall \epsilon > k$.
    L'algoritmo $\epsilon$-approssimante sarà di fatto l'algoritmo $k$-approssimante.
\end{rem}

\begin{defn}
    Un problema di approssimazione $\Pi$ è \textbf{APX-hard} se esiste una riduzione che preserva l'approssimazione \footnote{Vedere: Approxiamtion-preserving reduction, Wikipedia.} da qualsiasi problema in APX a $\Pi$.
\end{defn}

\begin{defn}
    Un problema è \textbf{APX-complete} se appartiene ad APX ed è APX-hard.\\
    Si possono considerare come i problemi più difficili da risolvere per la classe APX.
\end{defn}

\begin{rem}
    $\tn{PTAS} \subseteq \tn{APX}$\\
    Se $\tn{P} \neq \tn{NP}$, allora $\tn{PTAS} \subset \tn{APX}$, ovvero esistono problemi in APX che non sono PTAS, e $\tn{APX-complete} \neq \tn{APX} \setminus \tn{PTAS}$, ovvero esistono problemi in APX, ma non PTAS, che non sono APX-completi. Questi problemi prendono il nome di APX-intermediate.
\end{rem}

\subsection{Esempi}
\subsection*{Vertex Cover}
\subsubsection{Algoritmo $2$-approssimante per Vertex Cover}
L'algoritmo $2$-approssimante per il problema Vertex Cover è un algoritmo greedy che trova la copertura $C$ di un grafo $G$ calcolando il \textbf{matching massimale} \footnote{Il matching massimale di un grafo è l'insieme di archi senza vertici in comune che ha cardinalità massima.} di $G$. 

Per costruire il matching massimale, l'algoritmo considera tutti gli archi presenti in $E$. Estrae quindi un arco $(u,v)$ in maniera casuale, andando poi ad aggiungere i suoi estremi $u$ e $v$ al matching massimale. Rimuove poi da $E$ tutti gli archi cui almeno un estremo è $u$ o $v$. L'algoritmo itera fino a quando $E$ non è vuoto.

\begin{codebox}
    \Procname{$\proc{Vertex-Cover-Approx(V, E)}$}
    \li \While $E \neq $
    \End
\end{codebox}

\begin{thm}
    L'algoritmo \verb|Vertex-Cover-Approx| è $2$-approssimante, ovvero la copertura $C$ trovata dall'algoritmo di approssimazione contiene un numero di vertici $\appr(x) \le 2 \cdot \opt(x)$, con $\opt(x)$ numero di vertici contenuti nella copertura ottima.
\end{thm}

\begin{proof}
    DIMOSTRAZIONE
\end{proof}

\subsection*{Set Cover}
Il Set Cover Problem è un problema di ottimizzazione che chiede, dati un insieme universo $U$, una collezione di sottoinsiemi dell'insieme universo $S \subseteq \mathcal{P}(U)$ tale che $\bigcup\limits_{s \in S}^{\bullet} s = U$ e una funzione costo $\tn{c}: S \rightarrow \mathbb{Q}^+$, di trovare la set cover $C$ di $U$ di peso minimo, ovvero $C \subseteq S : \bigcup\limits_{c \in C}^{\bullet} c = U$ e il cui peso sia minimo.

Se il costo è uniforme, ovvero a ogni elemento di $S$ viene assegnato lo stesso costo, allora il problema chiede di trovare la set cover composta dal minor numero di elementi di $S$.

Si consideri ora il problema Set Cover nella sua versione di decisione.\\
\begin{problem}[lined]{Set Cover (di decisione)}
    Input: & \begin{minipage}[t]{0.8\linewidth}\begin{itemize}
        \setlength\itemsep{0em}
        \item un insieme universo $U$
        \item $S \subseteq \mathcal{P}(U)$ tale che $\bigcup\limits_{s \in S}^{\bullet} s = U$
        \item una funzione costo $\tn{c}: S \rightarrow \mathcal{Q}^+$
        \item $k \in Q^+$
    \end{itemize}\end{minipage}\\
    Output: & Esiste $C \subseteq S : \bigcup\limits_{e \in C}^{\bullet} e = U \land \sum\limits_{e \in C} \tn{c}(e) \le k$ ?
\end{problem}
\begin{thm}
    Il Set Cover Problem (di decisione) è un problema NP-completo.
\end{thm}
\begin{proof}
    La dimostrazione che Set Cover appartiene a NP si effettua dimostrando che un suo certificato può essere verificato in tempo polinomiale.\\
    Un certificato del problema è semplicemente una collezione di elementi di $S$. Il certificato cresce linearmente al crescere della cardinalità di $S$, e quindi ha dimensione polinomiale in funzione dell'input.\\
    La sua verifica avviene facilmente verificando che l'unione di tutti gli elementi di $S$ contenuti nel certificato sia l'insieme universo $U$ e che la somma dei loro costi sia minore o uguale al razionale positivo $k$ ricevuto in input. L'unione di due insiemi è polinomiale, e il confronto tra due numeri razionali è polinomiale. Di conseguenza, anche la verifica del certificato è polinomiale in funzione dell'input.

    Bisogna ora dimostrare che Set Cover è NP-hard. Per farlo basta trovare una riduzione da un problema NP-hard noto a Set Cover.\\
    Il problema NP-hard noto che si considera è Vertex Cover \footnote{Vertex Cover (di ottimizzazione) chiede, dato un grafo, di trovare un sottoinsieme di nodi tale che tutti gli archi del grafo abbiano almeno un estremo nel sottoinsieme di nodi scelto. Il problema si può facilmente riformulare come un problema di decisione.}. Un'istanza $(G = (E,V), j)$ di Vertex Cover può essere convertita in un'istanza $(U, S, \tn{c}, k)$ di Set Cover nel seguente modo:
    \begin{itemize}
        \item $U=E$ (avviene in tempo polinomiale, più precisamente costante);
        \item Supponendo di aver etichettato tutti i vertici in $V$ con $1 \le i \le |V|$, $S$ contiene tutti i sottoinsiemi $S_i \subseteq E : 1 \le i \le n$, dove $S_i$ contiene gli archi incidenti al vertice $i$ (avviene in tempo polinomiale, più precisamente $|V|^3$);
        \item la funzione di costo $\tn{c}$ viene definita come $\tn{c}(S_i) = 1$, in quanto considerare l'insieme $S_i$ significa prendere il vertice $i$ nella Vertex Cover del grafo $G$ (avviene in tempo polinomiale, più precisamente lineare in $|V|$);
        \item $k = j$ (avviene in tempo polinomiale, più precisamente costante).
    \end{itemize}
    
    L'istanza di Vertex Cover ha risposta positiva sse la relativa istanza di Set Cover ha risposta positiva.
    Infatti, se l'istanza di Vertex Cover ha una copertura $C$ con al più $j$ vertici, allora la copertura $C'$ per l'istanza di Set Cover è semplicemente quella formato dai sottoinsiemi $S_i, \forall i : i \in C$. Si può quindi affermare che $|C'| = |C|$. Ma siccome $|C| \le j \land k = j$, allora $|C'| \le k$.\\
    Viceversa, se l'istanza di Set Cover ammette una copertura $C', |C'| \le k$, allora l'istanza di Vertex Cover ammette copertura $C$ tale che $C = \cbra{i : 1 \le i \le |V| \land S_i \in C'}$, ovvero contiene tutti i vertici di indice $i$ per cui il relativo insieme di archi incidenti $S_i$ appartiene alla copertura $C'$. Di conseguenza, $|C| = |C'|$.
    Valendo inoltre $|C'| \le k$, e $j = k$, allora $|C| \le j$.

    Avendo dimostrato che Set Cover è in NP ed è NP-hard, allora Set Cover è un problema NP-completo.
\end{proof}

\subsection*{Travelling Salesman Problem simmetrico}
Il Travelling Salesman Problem simmetrico \footnote{Con simmetrico si intende che $\forall u, v \in V, d(u,v)=d(v,u)$, e quindi il grafo è non orientato.} (di ottimizzazione) chiede, dato un grafo non orientato pesato $G = (V, E, w)$, di trovare il ciclo Hamiltoniano di peso minore, se esiste.
\begin{problem}[lined]{Travelling Salesman Problem (di decisione)}
    Input: & \begin{minipage}[t]{0.8\linewidth}\begin{itemize}
        \setlength\itemsep{0em}
        \item un grafo $G = (V, E)$
        \item $k \in Q^+$
    \end{itemize}\end{minipage}\\
    Output: & Esiste un cammino hamiltoniano in $G$ di peso minore di $k$ ?
\end{problem}

Travelling Salesman Problem è un esempio di algoritmo senza un'approssimazione costante se $P \neq NP$: se TSP avesse un'approssimazione costante, allora Hamiltonian Path, un problema notoriamente NP completo, sarebbe risolvibile in tempo polinomiale e quindi $P = NP$, violando l'assunzione $P \neq NP$.

\begin{thm}
    Se $P \neq NP$, allora Travelling Salesman Problem (di ottimizzazione) non può avere un'approssimazione costante.
\end{thm}

\begin{proof}
    Si assuma esista un algoritmo $\epsilon$-approssimante per TSP e che $P \neq NP$.\\
    Sia $G = (V, E)$ un'istanza del problema Hamiltonian Path. Esistendo una riduzione Hamiltonian Path $\le_p$ Travelling Salesman Problem, è possibile costruire un'istanza di TSP a partire da $G$.
    Il grafo pesato $G' = (V', E', w)$ viene costruito come:
    \begin{itemize}
        \item $V' = V$
        \item $E' = V \times V$
        \item $\forall e \in E', \quad w(e) = \begin{cases}
            1 & e \in E\\
            \epsilon \cdot |V| + 1 & e \notin E
        \end{cases}$
    \end{itemize}
    Il valore ottimo $\opt(x)$ per l'algoritmo $\epsilon$-approssimante per TSP sarà $|V|$, mentre il valore della soluzione approssimata sarà, per definizione di algoritmo $\epsilon$-approssimante, minore di $\epsilon \cdot \opt(x)$.
    
    Nella risoluzione di TSP (di ottimizzazione) sul grafo $G'$ si possono verificare due casi:\\
    \textbf{Caso 1}: il cammino Hamiltoniano trovato dall'algoritmo $\epsilon$-approssimante include almeno uno degli archi di peso $\epsilon \cdot |V| + 1$. Di conseguenza, la soluzione approssimata di TSP ha costo $A(x) > \epsilon \cdot |V|$.\\
    Ma, per definizione di algoritmo $\epsilon$-approssimante, deve valere $|V| \le A(x) \le \epsilon \cdot |V|$. Di conseguenza, si ottiene una contraddizione, e non è possibile aver trovato un cammino Hamiltoniano che contiene un arco di peso $\epsilon \cdot |V| + 1$, ovvero uno degli archi aggiunti dalla riduzione.
    Di conseguenza, Hamiltonian Path sull'istanza $G$ non ammette cammino Hamiltoniano.
    
    \textbf{Caso 2}: il cammino Hamiltoniano trovato dall'algoritmo $\epsilon$-approssimante non include alcun arco di peso $\epsilon \cdot |V| + 1$, ma solo archi di peso $1$. Essendo il numero di archi in un cammino Hamiltoniano pari a $|V|$, allora il peso totale del cammino sarà $|V|$.\\
    Di conseguenza, vale $|V| \le A(x) \le \epsilon \cdot |V|$, in quanto $A(x) = |V|$.\\
    Ma essendo gli archi di peso $1$ archi che compaiono effettivamente nell'istanza di Hamiltonian Path, ed essendo il cammino Hamiltonanio trovato composto da soli archi di peso $1$, allora il cammino Hamiltoniano trovato con l'algoritmo $\epsilon$-approssimante di TSP è soluzione anche per Hamiltonian Path.

    Si può notare che l'algoritmo appena descritto si comporta di fatto come un algoritmo di decisione per il problema Hamiltonian Path, ed ha complessità polinomiale.
    Ma Hamiltonian Path è in NP e, quindi, si ottiene $P = NP$, violando l'ipotesi $P \neq NP$.
\end{proof}

\subsection*{Travelling Salesman Problem metrico}
Il Travelling Salesman Problem metrico è un tipo particolare di Travelling Salesman Problem in cui si considerano solo grafi simmetrici (e quindi non orientati) pesati in cui vale la disuguaglianza triangolare \footnote{Con disuguaglianza triangolare si intende $\forall u, v, z \in V, d(u,v) + d(v,z) \ge d(u,z)$}.

Travelling Salesman Problem metrico, al contrario del Travelling Salesman Problem simmetrico generico, ammette un algoritmo di approssimazione $\epsilon$-approssimante. Uno dei migliori algoritmi di approssimazione conosciuti per TSP metrico è l'algoritmo di Christofides, $\frac{3}{2}$-approssimante.
Esiste però un algoritmo più semplice e abbastanza intuitivo $2$-approssimante.

\subsubsection{Algoritmo $2$-approssimante per TSP metrico}
L'algoritmo $2$-approssimante per TSP metrico qui descritto prevede di costruire un Minimum Spanning Tree \footnote{Il Minimum Spanning Tree di un grafo è un albero che copre tutti i nodi del grafo ed ha costo minimo.} per il grafo, per poi costruire il ciclo Hamiltoniano a partire dal MST.

Innanzitutto bisogna trovare una relazione tra il costo del MST trovato, ovvero il suo peso, e il costo del ciclo Hamiltoniano ottimo del grafo $G$, ovvero il suo costo.
\begin{thm}
    Sia $T$ l'MST del grafo $G$, $c(T)$ il suo costo e $\opt(G)$ il costo del cammino Hamiltoniano ottimo di $G$, allora vale:
\end{thm}
\[
    c(T) \le \opt(G)
\]
\begin{proof}
    Se si considera un tour \footnote{Un tour è un ciclo che attraversa tutti i nodi del grafo, ma non è detto che lo faccia una e una sola volta per nodo. Il ciclo Hamiltoniano è un tipo particolare di tour, in cui ogni nodo è visitato esattamente una volta.} di $G$ e vi si rimuove un arco, allora si ottiene un albero di copertura, non necessariamente minimo (ovvero MST).

    Di conseguenza, supponendo che $T_1$ sia l'MST e $H$ sia il ciclo Hamiltoniano ottimo (che è un tour), togliendo un arco da $H$ si ottiene un nuovo albero di copertura $T_2$ di costo $c(T_2)$ e vale $c(T_2) \le c(H)$, in quanto si è rimosso un arco. Ma $T_2$ non è necessariamente albero di copertura minimo (lo è se $T_1 = T_2$, avendo assunto $T_1$ MST), e quindi vale $c(T_1) \le c(T_2)$.\\
    Di conseguenza vale $c(T_1) \le c(T_2) \le c(H)$ e quindi $c(T_1) \le c(H)$.
\end{proof}

L'MST del grafo viene costruito utilizzando l'algoritmo di Prim o l'algoritmo di Kruscal, entrambi di tempo polinomiale.\\
Una volta costruito l'MST $T$, allora $c(T) \le \opt(G)$ per il precedente teorema.

Bisogna ora trovare un modo di costruire un cammino Hamiltoniano $H$ a partire dal MST $T$ appena trovato.\\
Per farlo, si costruisce un grafo contenente tutti gli archi di $T$ duplicati, anche in questo caso in tempo polinomiale.
Si può notare che su questo grafo è possibile definire un ciclo euleriano \footnote{Un grafo $G$ ammette un ciclo euleriano sse tutti i nodi hanno grado pari.} $E$, ovvero un ciclo in cui si visitano tutti gli archi esattamente una volta. Il costo del ciclo euleriano $E$ sarà 
\[
    c(E) = 2 \cdot c(T)
\]

Il ciclo euleriano $E$ appena ottenuto non è però un ciclo Hamiltoniano. Per ottenere il ciclo Hamiltoniano $H$, che sarà il risultato dell'algoritmo di approssimazione, basta rimuovere dalla sequenza di nodi che descrive il ciclo euleriano i vertici già visitati, tranne l'ultimo elemento che chiude il ciclo.

\begin{thm}
    Sia $c(H)$ il costo del ciclo Hamiltoniano individuato dall'algoritmo di approssimazione e $c(E)$ il costo del ciclo euleriano da cui $H$ è stato generato.   \[
        c(H) \le c(E)
    \]
\end{thm}
\begin{proof}
    Nel TSP metrico, che è il caso di TSP che si sta considerando, vale la disuguaglianza triangolare.
    Se quindi, quando rimuovo nodi già visitati, sostituisco un cammino $u \leadsto v$ composto da più archi con l'arco che collega direttamente $u$ e $v$, allora il nuovo cammino avrà sicuramente costo inferiore, proprio perchè vale la disuguaglianza triangolare. 

    Di conseguenza, tutti i nodi già esplorati che vengono rimossi e la definizione di un collegamento diretto portano a un ciclo (Hamiltoniano) sicuramente di costo minore o uguale al ciclo euleriano.
\end{proof}

Sia quindi $H^*$ il ciclo Hamiltoniano ottimo, $H$ il ciclo Hamiltoniano trovato dall'algoritmo di approssimazione, $T$ l'MST e $E$ il ciclo euleriano definiti nel corso dell'esecuzione dell'algoritmo di approssimazione. Vale:
\[
    c(H) \le c(E) = 2 \cdot c(T) \le 2 \cdot c(H*^)
\]
Di conseguenza:
\[
    c(H^*) \le c(H) \le 2 \cdot c(H^*)
\]
Il rapporto di approssimazione di questo algoritmo di approssimazione è quindi costante e pari a $\epsilon = 2$.

\subsection*{Travelling Salesman Problem asimmetrico}

\subsection*{Travelling Purchaser Problem}

\subsection*{Consensus Sequence}
Consensus Sequence è il problema che chiede, data una collezione di stringhe, di trovare la stringa che minimizza la somma delle distanze di edit \footnote{La distanza di edit $d(s^{\prime}, s^{\prime\prime})$ è il numero di operazioni di inserimento, rimozione e sostituzione necessarie per ottenere $s^{\prime\prime}$ a partire da $s^{\prime}$.} da tutte le stringhe della collezione.

\begin{problem}[lined]{Consensus Sequence}
    Input: & \begin{minipage}[t]{0.8\linewidth}\begin{itemize}
        \setlength\itemsep{0em}
        \item una collezione di $k$ stringhe $S = \cbra{s_1, \ldots, s_k}$
    \end{itemize}\end{minipage}\\
    Output: & Stringa $s^*$ tale che $\sum\limits_{i=1}^k d(s^*, s_i)$ è minima.
\end{problem}

La stringa $s^*$ non deve necessariamente appartenere a $S$. 